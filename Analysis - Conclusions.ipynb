{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis - Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages and Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Numeric computing\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "# Importing data sets\n",
    "from sklearn import datasets\n",
    "# Splitting data sets randomly into training and test sets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "# Graphing and images\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Inputting latex\n",
    "from IPython.display import display, Math, Latex\n",
    "# Package for timing\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iris Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "irisInput = iris.data\n",
    "irisTarget = iris.target\n",
    "print( \"The Iris dataset gives a vector of four dimensions to predict one of three classes\")\n",
    "print( \"An example of the iris input is\") \n",
    "print( irisInput[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# xNorm normalises the data to between -1 and 1\n",
    "def xNorm(data):\n",
    "    data_ = np.transpose(data)\n",
    "    for i in range(len(data_)):\n",
    "        max_ = max(data_[i]); min_ = min(data_[i])\n",
    "        if (max_ - min_) == 0:\n",
    "            data_[i] = -1\n",
    "        else:\n",
    "            data_[i] = [2*float(data_[i][j] - min_)/float(max_ - min_) - 1 \\\n",
    "                    for j in range(len(data_[i]))]\n",
    "    return np.transpose(data_)\n",
    "\n",
    "# yNorm uses one-to-many normalization to return a n unit vector s.t.\n",
    "# yNorm(2) -> [0,1,0,0,0,0,0,0,0]\n",
    "def yNorm(data):\n",
    "    dataDim = len(data)\n",
    "    yData = np.zeros((dataDim,max(data)+1))\n",
    "    for i in range(dataDim):\n",
    "        yData[i][data[i]-1] = 1\n",
    "    return yData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBF Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# r calculates the Euclidean Distance between vectors\n",
    "def r(x,centre):\n",
    "    if isinstance(x,float):\n",
    "        return abs(x - centre)\n",
    "    else:\n",
    "        return np.sqrt(sum([(x[i]-centre[i])**2 for i in range(len(x))]))\n",
    "\n",
    "# Scaling Parameter\n",
    "def sigma(data,centre):\n",
    "    m = len(data)\n",
    "    return float(1)/float(m)*sum([r(data[i], centre) for i in range(m)])\n",
    "\n",
    "# Calculates beta for the required activaion function\n",
    "def betaCalc(data, centres):\n",
    "    beta = np.zeros(len(centres))\n",
    "    for i in range(len(centres)):\n",
    "        sigma_ = sigma(data, centres[i])\n",
    "        beta[i] = float(1) / float(2*sigma_**2)\n",
    "    return beta\n",
    "\n",
    "# Gaussian activation function\n",
    "def hGauss(x, centre, beta):\n",
    "    return np.exp(-beta*r(x,centre)**2)\n",
    "\n",
    "# Ricker Wavelet activation function\n",
    "def hRickerWavelet(x, centre, beta):\n",
    "    return (1 - r(x,centre)**2)*np.exp(-beta*r(x,centre)**2/2)\n",
    "\n",
    "# Plotting Examples of both activation functions\n",
    "xExVals = np.linspace(-5,5,1000)\n",
    "yExGaussVals = [hGauss(x,0,1) for x in xExVals]\n",
    "yExRWVals = [hRickerWavelet(x, 0, 1) for x in xExVals]\n",
    "print(\"The two RBF activation functions are\")\n",
    "display(Math(r'hGauss(r) = e^{-\\beta r^2}'))\n",
    "plt.plot(xExVals, yExGaussVals); plt.show()\n",
    "display(Math(r'hRickerWavelet(r) = (1-r)e^{\\frac{-\\beta r^2}{2}}'))\n",
    "plt.plot(xExVals, yExRWVals); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBF Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Complete RBF Network\n",
    "def f(x,beta,w,centres, actFunct):\n",
    "    if actFunct == \"RW\":\n",
    "        h = hRickerWavelet\n",
    "    else:\n",
    "        h = hGauss\n",
    "    F = np.zeros(len(w))\n",
    "    for j in range(len(F)):\n",
    "        for i in range(len(centres)):\n",
    "            F[j] += w[j][i]*h(x, centres[i], beta[i])\n",
    "    return F\n",
    "\n",
    "# Classification function\n",
    "# classFunct([1.5 32 5 2 7]) --> [0 1 0 0 0]\n",
    "def classFunct(F):\n",
    "    y_ = np.zeros(len(F))\n",
    "    y_[np.argmax(F)] = 1\n",
    "    return y_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Euclidean Distance Measure\n",
    "def dist(point1, point2):\n",
    "    n = 0\n",
    "    if isinstance(point1, int):\n",
    "        n = abs(point2 - point1)\n",
    "    else:\n",
    "        for i in range(len(point1)): \n",
    "            n += abs(point2[i] - point1[i])\n",
    "    return n\n",
    "\n",
    "# Randomly assigns the data into n separate clusters at initiation\n",
    "def initKMeans(data,dim):\n",
    "    clusters = [[] for _ in range (dim)]\n",
    "    dataIndex = range(len(data))\n",
    "    dataIndex = sorted(dataIndex, key=lambda k:rnd.random())\n",
    "    split = int(round(len(data)/dim)); n = 0\n",
    "    for i in range(len(clusters)):\n",
    "        clusters[i] = [data[j] for j in dataIndex[n:n+split]]\n",
    "        n += split\n",
    "    dif = len(data) - n\n",
    "    clusters[-1] += [data[j] for j in dataIndex[-dif:]]\n",
    "    return clusters\n",
    "\n",
    "# Updates the centres of the clusters after every iteration\n",
    "def kMeanCentUpdate(clusters,data):\n",
    "    centres = [[] for _ in range(len(clusters))]\n",
    "    for i in range(len(clusters)):\n",
    "        if isinstance(data[0], int):\n",
    "            centres[i] =  np.mean(clusters[i])\n",
    "        elif len(clusters[i]) == 0:\n",
    "            centres[i] = [[0] for _ in range(len(data[0]))]\n",
    "        else:\n",
    "            centres[i] = [[] for _ in range(len(data[0]))]\n",
    "            for j in range(len(centres[i])):\n",
    "                targetArray = [clusters[i][x][j] for x in range(len(clusters[i]))]\n",
    "                centres[i][j] = np.mean(targetArray)\n",
    "    return centres\n",
    "\n",
    "# Updates the data, placing each data point in closest nearest cluster\n",
    "def kMeanUpdate(clusters, centres):\n",
    "    nClust = [[] for _ in range(len(clusters))]\n",
    "    for i in range(len(clusters)):\n",
    "        for j in range(len(clusters[i])):\n",
    "            distArray = [dist(clusters[i][j], centres[x]) for x in range(len(centres))]\n",
    "            optCluster = np.argmin(distArray)\n",
    "            nClust[optCluster] += [clusters[i][j]]\n",
    "    return nClust\n",
    "\n",
    "# Complete K-Means algorithm\n",
    "def kMeans(data, dim):\n",
    "    clusters = initKMeans(data,dim) \n",
    "    oldCentres = None; centres = 0;\n",
    "    n = 0\n",
    "    while centres != oldCentres and n < 20:\n",
    "        oldCentres = centres\n",
    "        centres = kMeanCentUpdate(clusters,data) \n",
    "        clusters = kMeanUpdate(clusters, centres)\n",
    "        n += 1\n",
    "    return clusters, centres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = [irisInput[i][0] for i in range(len(irisInput))]\n",
    "Y = [irisInput[i][1] for i in range(len(irisInput))]\n",
    "print(\"Here we will use the iris dataset to give an example of k-Means clustering\")\n",
    "plt.plot(X,Y,'o')\n",
    "plt.show()\n",
    "trainData = np.column_stack((X,Y))\n",
    "clusters,_ = kMeans(trainData,2)\n",
    "x1 = [clusters[0][i][0] for i in range(len(clusters[0]))]\n",
    "y1 = [clusters[0][i][1] for i in range(len(clusters[0]))]\n",
    "x2 = [clusters[1][i][0] for i in range(len(clusters[1]))]\n",
    "y2 = [clusters[1][i][1] for i in range(len(clusters[1]))]\n",
    "plt.plot(x1,y1,'o')\n",
    "plt.plot(x2,y2,'o')\n",
    "print(\"The data split into two clusters\")\n",
    "plt.show()\n",
    "clusters,_ = kMeans(trainData,3)\n",
    "x_1 = [clusters[0][i][0] for i in range(len(clusters[0]))]\n",
    "y_1 = [clusters[0][i][1] for i in range(len(clusters[0]))]\n",
    "x_2 = [clusters[1][i][0] for i in range(len(clusters[1]))]\n",
    "y_2 = [clusters[1][i][1] for i in range(len(clusters[1]))]\n",
    "x_3 = [clusters[2][i][0] for i in range(len(clusters[2]))]\n",
    "y_3 = [clusters[2][i][1] for i in range(len(clusters[2]))]\n",
    "plt.plot(x_1,y_1,'o')\n",
    "plt.plot(x_2,y_2,'o')\n",
    "plt.plot(x_3,y_3,'o')\n",
    "print(\"The data split into three clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print( \"The two loss functions we will look at are\")\n",
    "display(Math(r'loss(x) = \\frac{1}{m}\\Bigg(\\sum_{i=1}^{m} \\bigg({\\begin{cases}{0}& \\text{if }f(x^i) = y^i\\\\{1} & \\text{else}\\\\\\end{cases}}\\bigg)\\Bigg)'))\n",
    "display(Math(r'logLoss(x) = \\frac{1}{m}(\\sum_{i=1}^{m}y^{i}log(f(x^{i})) + ' \\\n",
    "             + '(1-y^{i})log(1-f(x^i))'))\n",
    "\n",
    "def loss(w, beta, centres, trainData, trainTarget,actFunct):\n",
    "    cost = 0\n",
    "    for i in range(len(trainData)):\n",
    "        cost += sum([abs(classFunct(f(trainData[i], beta, w, centres,actFunct))[j] - \\\n",
    "                         trainTarget[i][j]) for j in range(len(trainTarget[i]))])\n",
    "    return float(cost)/float(2*len(trainData))\n",
    "\n",
    "def logLoss(w, beta, centres, trainData, trainTarget,actFunct):\n",
    "    cost = 0\n",
    "    for i in range(len(trainData)):\n",
    "        cost += abs(sum([(trainTarget[i][j]*np.log(f(trainData[i],beta,w,centres,actFunct)[j]) + \\\n",
    "                 (1 - trainTarget[i][j])*np.log(f(trainData[i],beta,w,centres,actFunct)[j])) \\\n",
    "                 for j in range(len(trainTarget[i]))]))\n",
    "    return float(cost) / float(len(trainData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradDescent(weighting, beta, centres, trainData, targetData, learnRate, actFunct):\n",
    "    if actFunct == \"RW\":\n",
    "        h = hRickerWavelet\n",
    "    else:\n",
    "        h = hGauss\n",
    "    wGrad = np.zeros((len(weighting),len(weighting[0])))\n",
    "    for point in range(len(trainData)):\n",
    "        F = f(trainData[point],beta,weighting,centres,actFunct)\n",
    "        Y = targetData[point]\n",
    "        diff = [tar - pred for pred,tar in zip(F,Y)]\n",
    "        for i in range(len(wGrad)):\n",
    "            for j in range(len(wGrad[1])):\n",
    "                wGrad[i][j] += -h(trainData[point], centres[j], beta[j])*diff[i]\n",
    "    w_ = np.zeros((len(weighting),len(weighting[1])))\n",
    "    for i in range(len(weighting)):\n",
    "        w_[i] = [origW - learnRate*wIter for origW,wIter in zip(weighting[i],wGrad[i])]\n",
    "    return w_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradDescentM(weighting, beta, centres, trainData, targetData, learnRate, eps, velocity,\\\n",
    "                 actFunct):\n",
    "    wGrad = np.zeros((len(weighting),len(weighting[0])))\n",
    "    if actFunct == \"RW\":\n",
    "        h = hRickerWavelet\n",
    "    else:\n",
    "        h = hGauss\n",
    "    wGrad = np.zeros((len(weighting),len(weighting[0])))\n",
    "    for point in range(len(trainData)):\n",
    "        F = f(trainData[point],beta,weighting,centres,actFunct)\n",
    "        Y = targetData[point]\n",
    "        diff = [tar - pred for pred,tar in zip(F,Y)]\n",
    "        for i in range(len(wGrad)):\n",
    "            for j in range(len(wGrad[1])):\n",
    "                wGrad[i][j] += -h(trainData[point], centres[j], beta[j])*diff[i]\n",
    "                velocity[i][j] = learnRate*velocity[i][j] - eps*wGrad[i][j]\n",
    "        w_ = np.zeros((len(weighting),len(weighting[1])))\n",
    "    for i in range(len(weighting)):\n",
    "        w_[i] = [origW - v for origW,v in zip(weighting[i],velocity[i])]\n",
    "    return w_, velocity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent Optimisation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def optimiseGradDesc(weighting,beta,centres,trainData,targetData,learnRate, \\\n",
    "                     maxTime,actFunct):\n",
    "    iterations = 0; TOL = 10e10; optList = [10e10, None, None]\n",
    "    h = []; n = 1\n",
    "    startTime = time.time()\n",
    "    while time.time() < (startTime + maxTime): \n",
    "        weighting=gradDescent(weighting,beta,centres,trainData,targetData,\\\n",
    "                              learnRate,actFunct)\n",
    "        TOL = loss(weighting,beta,centres,trainData,targetData,actFunct)\n",
    "        iterations += 1 \n",
    "        elapsedTime = time.time() - startTime\n",
    "        if elapsedTime > (n*maxTime)/5:\n",
    "            print(\"At \" + str(round(elapsedTime,4)) +\" seconds and \"+str(iterations) + \\\n",
    "                  \" iterations, the TOL is \" + str(round(TOL,6)))\n",
    "            n += 1\n",
    "        if optList[0] > TOL:\n",
    "            optList = [TOL, iterations, weighting]\n",
    "        if (optList[1]+300) < iterations:\n",
    "            print(\"RBF converged to min at iteration n = \"+str(optList[1])+\" with a \"\\\n",
    "                  + \"TOL of \" + str(round(optList[0],6)))\n",
    "            return optList, h\n",
    "        h += [[elapsedTime, TOL]]\n",
    "    print(\"Optimal optimisation at n = \"+str(optList[1])+\" of TOL: \"\\\n",
    "          +str(round(optList[0],6)))\n",
    "    return optList, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def optimiseSGD(weighting,beta,centres,trainData,targetData,learnRate,batchSize, \\\n",
    "                maxTime,actFunct):\n",
    "    iterations = 0; TOL = 10e10; optList = [10e10, None, None]\n",
    "    h = []; n = 10e10; startTime = time.time(); m = 1\n",
    "    while time.time() < (startTime + maxTime):\n",
    "        if (n+batchSize - len(trainData)) > 0:\n",
    "            trainDataIndex = range(len(trainData))\n",
    "            rnd.shuffle(trainDataIndex)\n",
    "            trainData = [trainData[i] for i in trainDataIndex]\n",
    "            targetData = [targetData[i] for i in trainDataIndex]\n",
    "            n = 0\n",
    "        weighting = gradDescent(weighting,beta,centres,trainData[n:n+batchSize], \\\n",
    "                                targetData[n:n+batchSize],learnRate,actFunct)\n",
    "        TOL = loss(weighting,beta,centres,trainData,targetData,actFunct)\n",
    "        iterations += 1; n += batchSize\n",
    "        elapsedTime = time.time() - startTime\n",
    "        if elapsedTime > (m*maxTime)/5:\n",
    "            print(\"At \" + str(round(elapsedTime,4)) +\" seconds and \"+str(iterations) + \\\n",
    "                  \" iterations, the TOL is \" + str(round(TOL,6)))\n",
    "            m += 1\n",
    "        if optList[0] > TOL:\n",
    "            optList = [TOL, iterations, weighting]\n",
    "        if (optList[1]+250) < iterations:\n",
    "            print(\"RBF converged to min at iteration = \"+str(optList[1])+\" with a TOL of \"\\\n",
    "                 + str(round(optList[0],6)))\n",
    "            return optList, h\n",
    "        h += [[elapsedTime, TOL]]\n",
    "    print(\"Optimal optimisation at n = \"+str(optList[1])+\" of TOL: \"\\\n",
    "          +str(round(optList[0],6)))\n",
    "    return optList, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimiseSGD_M(weighting,beta,centres,trainData,targetData,learnRate,eps,batchSize, \\\n",
    "                maxTime,actFunct):\n",
    "    iterations = 0; TOL = 10e10; optList = [10e10, None, None]\n",
    "    h = []; n = 10e10; startTime = time.time(); m = 1\n",
    "    velocity = np.random.rand(len(weighting),len(weighting[0]))\n",
    "    while time.time() < (startTime + maxTime):\n",
    "        if (n+batchSize - len(trainData)) > 0:\n",
    "            trainDataIndex = range(len(trainData))\n",
    "            rnd.shuffle(trainDataIndex)\n",
    "            trainData = [trainData[i] for i in trainDataIndex]\n",
    "            targetData = [targetData[i] for i in trainDataIndex]\n",
    "            n = 0\n",
    "        weighting, velocity = gradDescentM(weighting,beta,centres,trainData[n:n+batchSize],\\\n",
    "                                targetData[n:n+batchSize],learnRate,eps,velocity,actFunct)\n",
    "        TOL = loss(weighting,beta,centres,trainData,targetData,actFunct)\n",
    "        iterations += 1; n += batchSize\n",
    "        elapsedTime = time.time() - startTime\n",
    "        if elapsedTime > (m*maxTime)/5:\n",
    "            print(\"At \" + str(round(elapsedTime,4)) +\" seconds and \"+str(iterations) + \\\n",
    "                  \" iterations, the TOL is \" + str(round(TOL,6)))\n",
    "            m += 1\n",
    "        if optList[0] > TOL:\n",
    "            optList = [TOL, iterations, weighting]\n",
    "        if (optList[1]+250) < iterations:\n",
    "            print(\"RBF converged to min at iteration = \"+str(optList[1])+\" with a TOL of \"\\\n",
    "                 + str(round(optList[0],6)))\n",
    "            return optList, h\n",
    "        h += [[elapsedTime, TOL]]\n",
    "    print(\"Optimal optimisation at n = \"+str(optList[1])+\" of TOL: \"\\\n",
    "          +str(round(optList[0],6)))\n",
    "    return optList, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBF Network Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Percentage Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function measures the accuracy of the RBF Network\n",
    "def classPercent(weighting,beta,centres,xData,yData,actFunct):\n",
    "    correct = 0\n",
    "    M = len(xData)\n",
    "    for i in range(M):\n",
    "        y = yData[i]\n",
    "        y_ = classFunct(f(xData[i],beta,weighting,centres,actFunct))\n",
    "        if (y_ == y).all():\n",
    "            correct += 1\n",
    "    percentage = round((float(correct)/float(M))*100,3)\n",
    "    print(\"RBF correctly identified \"+str(correct)+\" out of \"+str(M)+\" points\")\n",
    "    print(\"This gives an accuracy of \"+str(percentage)+\"% \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Opt Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def completeRBFNet(trainData, targetData, rbfIndex, learnRate, eps, batchSize,maxIter):\n",
    "    # Splits the data into a training and test set\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(trainData, targetData, test_size=0.33, \\\n",
    "                                               random_state = 42)\n",
    "    # Normalize data\n",
    "    xTrain_ = xNorm(xTrain); xTest_ = xNorm(xTest)\n",
    "    yTrain_ = yNorm(yTrain); yTest_ = yNorm(yTest)\n",
    "    # Defining the rbf centres and beta value\n",
    "    _,centres = kMeans(xTrain_, rbfIndex)\n",
    "    beta = betaCalc(xTrain_, centres)\n",
    "    # Defining the initial weighting matrix\n",
    "    yLength = len(yTrain_[0])\n",
    "    weighting = np.random.random((yLength, rbfIndex))\n",
    "    # Optimising the RBF weight matrix\n",
    "    print(\"Optimising the RBF weights\")\n",
    "    print(\"\")\n",
    "    minTOL = 0.01\n",
    "    print(\"Deterministic Gradient Descent\")\n",
    "    optCondGD,optListGD = optimiseGradDesc(weighting,beta,centres,xTrain_,yTrain_,\\\n",
    "                                           learnRate[0],maxIter,\"Gauss\")\n",
    "    optWeightsGD = optCondGD[2]\n",
    "    print(\"\")\n",
    "    print(\"Stochastic Gradient Descent w. Momentum\")\n",
    "    optCondSGD,optListSGD = optimiseSGD(weighting,beta,centres,xTrain_,yTrain_,\\\n",
    "                                           learnRate[1],batchSize,maxIter,\"Gauss\")\n",
    "    optWeightsSGD = optCondSGD[2]\n",
    "    # Plotting a graph of iterations against TOL value for both Gaussian and Ricker Wavelet\n",
    "    # activation functions\n",
    "    optListGDIndex = range(len(optListGD)) \n",
    "    nValsGD = [optListGD[i][0] for i in optListGDIndex]\n",
    "    TOLValsGD = [optListGD[i][1] for i in optListGDIndex]\n",
    "    plt.plot(nValsGD, TOLValsGD, 'r')\n",
    "    optListSGDIndex = range(len(optListSGD)) \n",
    "    nValsSGD = [optListSGD[i][0] for i in optListSGDIndex]\n",
    "    TOLValsSGD = [optListSGD[i][1] for i in optListSGDIndex]\n",
    "    plt.plot(nValsSGD, TOLValsSGD, 'g')\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"TOL\")\n",
    "    plt.show()\n",
    "    # Looking at the network's accuracy\n",
    "    print(\"\")\n",
    "    print(\"For Deterministic\")\n",
    "    print(\"Training data:\")\n",
    "    classPercent(optWeightsGD, beta, centres, xTrain_, yTrain_, \"Gauss\")\n",
    "    print(\"Test data:\")\n",
    "    classPercent(optWeightsGD, beta, centres, xTest_, yTest_, \"Gauss\")\n",
    "    print(\"\")\n",
    "    print(\"For SGD w. Momentum\")\n",
    "    print(\"Training data:\")\n",
    "    classPercent(optWeightsSGD, beta, centres, xTrain_, yTrain_, \"Gauss\")\n",
    "    print(\"Test data:\")\n",
    "    classPercent(optWeightsSGD, beta, centres, xTest_, yTest_, \"Gauss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "completeRBFNet(irisInput, irisTarget, 15, [5e-4,5e-3,5e-3], 0.99, 10,60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
