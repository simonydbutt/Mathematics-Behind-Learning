{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages and Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Numeric computing\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "# Importing data sets\n",
    "import os\n",
    "from numpy import genfromtxt\n",
    "# Graphing and images\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Inputting latex\n",
    "from IPython.display import display, Math, Latex\n",
    "# Package for timing\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataTrain = genfromtxt('MNIST/train.csv', delimiter=',')\n",
    "dataTest = genfromtxt('MNIST/test.csv',delimiter=',')\n",
    "# Splits data into [x_i , y_i] \n",
    "def dataSplit(data):\n",
    "    return [[data[i][1:],data[i][0]] for i in range(1,len(data))]\n",
    "trainMNIST = dataSplit(dataTrain)\n",
    "testMNIST = dataSplit(dataTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# xNorm normalises the data to between -1 and 1\n",
    "def dimensionReduction(data, newSize):\n",
    "    xData_ = []; yData = []\n",
    "    for x in range(len(data)):\n",
    "        img = np.reshape(data[x][0],(28,28))\n",
    "        imgReduced = np.zeros((7,7))\n",
    "        for i in range(len(imgReduced)):\n",
    "            dataAvg = [(img[4*i][4*j] + img[4*i+1][4*j] + img[4*i][4*j+1] + \n",
    "                        img[4*i+1][4*j+1]) / 4 for j in range(len(imgReduced[0]))]\n",
    "            imgReduced[i] = dataAvg\n",
    "        imgReduced_ = np.reshape(imgReduced,(1,49))[0]\n",
    "        xData_ += [imgReduced_]\n",
    "        yData += [data[x][1]]\n",
    "        condIndex = np.random.choice(len(xData_),newSize)\n",
    "    xDataCond = xData_[condIndex]\n",
    "    yDataCond = yData[condIndex]\n",
    "    return xDataCond, yDataCond\n",
    "\n",
    "def xNorm(data):\n",
    "    data_ = np.transpose(data)\n",
    "    for i in range(len(data_)):\n",
    "        max_ = max(data_[i]); min_ = min(data_[i])\n",
    "        if (max_ - min_) == 0:\n",
    "            data_[i] = -1\n",
    "        else:\n",
    "            data_[i] = [2*float(data_[i][j] - min_)/float(max_ - min_) - 1 \\\n",
    "                    for j in range(len(data_[i]))]\n",
    "    return np.transpose(data_)\n",
    "\n",
    "def yNorm(data):\n",
    "    dataDim = len(data)\n",
    "    yData = np.zeros((dataDim,max(data)+1))\n",
    "    for i in range(dataDim):\n",
    "        yData[i][data[i]-1] = 1\n",
    "    return yData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBF Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# r calculates the Euclidean Distance between vectors\n",
    "def r(x,centre):\n",
    "    if isinstance(x,float):\n",
    "        return abs(x - centre)\n",
    "    else:\n",
    "        return np.sqrt(sum([(x[i]-centre[i])**2 for i in range(len(x))]))\n",
    "\n",
    "# Scaling Parameter\n",
    "def sigma(data,centre):\n",
    "    m = len(data)\n",
    "    return float(1)/float(m)*sum([r(data[i], centre) for i in range(m)])\n",
    "\n",
    "# Calculates beta for the required activaion function\n",
    "def betaCalc(data, centres):\n",
    "    beta = np.zeros(len(centres))\n",
    "    for i in range(len(centres)):\n",
    "        sigma_ = sigma(data, centres[i])\n",
    "        beta[i] = float(1) / float(2*sigma_**2)\n",
    "    return beta\n",
    "\n",
    "# Gaussian activation function\n",
    "def hGauss(x, centre, beta):\n",
    "    return np.exp(-beta*r(x,centre)**2)\n",
    "\n",
    "# Ricker Wavelet activation function\n",
    "def hRickerWavelet(x, centre, beta):\n",
    "    return (1 - r(x,centre)**2)*np.exp(-beta*r(x,centre)**2/2)\n",
    "\n",
    "# Plotting Examples of both activation functions\n",
    "xExVals = np.linspace(-5,5,1000)\n",
    "yExGaussVals = [hGauss(x,0,1) for x in xExVals]\n",
    "yExRWVals = [hRickerWavelet(x, 0, 1) for x in xExVals]\n",
    "print(\"The two RBF activation functions are\")\n",
    "display(Math(r'hGauss(r) = e^{-\\beta r^2}'))\n",
    "plt.plot(xExVals, yExGaussVals); plt.show()\n",
    "display(Math(r'hRickerWavelet(r) = (1-r)e^{\\frac{-\\beta r^2}{2}}'))\n",
    "plt.plot(xExVals, yExRWVals); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBF Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Complete RBF Network\n",
    "def f(x,beta,w,centres, actFunct):\n",
    "    if actFunct == \"RW\":\n",
    "        h = hRickerWavelet\n",
    "    else:\n",
    "        h = hGauss\n",
    "    F = np.zeros(len(w))\n",
    "    for j in range(len(F)):\n",
    "        for i in range(len(centres)):\n",
    "            F[j] += w[j][i]*h(x, centres[i], beta[i])\n",
    "    return F\n",
    "\n",
    "# Classification function\n",
    "# classFunct([1.5 32 5 2 7]) --> [0 1 0 0 0]\n",
    "def classFunct(F):\n",
    "    y_ = np.zeros(len(F))\n",
    "    y_[np.argmax(F)] = 1\n",
    "    return y_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Euclidean Distance Measure\n",
    "def dist(point1, point2):\n",
    "    n = 0\n",
    "    if isinstance(point1, int):\n",
    "        n = abs(point2 - point1)\n",
    "    else:\n",
    "        for i in range(len(point1)): \n",
    "            n += abs(point2[i] - point1[i])\n",
    "    return n\n",
    "\n",
    "# Randomly assigns the data into n separate clusters at initiation\n",
    "def initKMeans(data,dim):\n",
    "    clusters = [[] for _ in range (dim)]\n",
    "    dataIndex = range(len(data))\n",
    "    dataIndex = sorted(dataIndex, key=lambda k:rnd.random())\n",
    "    split = int(round(len(data)/dim)); n = 0\n",
    "    for i in range(len(clusters)):\n",
    "        clusters[i] = [data[j] for j in dataIndex[n:n+split]]\n",
    "        n += split\n",
    "    dif = len(data) - n\n",
    "    clusters[-1] += [data[j] for j in dataIndex[-dif:]]\n",
    "    return clusters\n",
    "\n",
    "# Updates the centres of the clusters after every iteration\n",
    "def kMeanCentUpdate(clusters,data):\n",
    "    centres = [[] for _ in range(len(clusters))]\n",
    "    for i in range(len(clusters)):\n",
    "        if isinstance(data[0], int):\n",
    "            centres[i] =  np.mean(clusters[i])\n",
    "        elif len(clusters[i]) == 0:\n",
    "            centres[i] = [[0] for _ in range(len(data[0]))]\n",
    "        else:\n",
    "            centres[i] = [[] for _ in range(len(data[0]))]\n",
    "            for j in range(len(centres[i])):\n",
    "                targetArray = [clusters[i][x][j] for x in range(len(clusters[i]))]\n",
    "                centres[i][j] = np.mean(targetArray)\n",
    "    return centres\n",
    "\n",
    "# Updates the data, placing each data point in closest nearest cluster\n",
    "def kMeanUpdate(clusters, centres):\n",
    "    nClust = [[] for _ in range(len(clusters))]\n",
    "    for i in range(len(clusters)):\n",
    "        for j in range(len(clusters[i])):\n",
    "            distArray = [dist(clusters[i][j], centres[x]) for x in range(len(centres))]\n",
    "            optCluster = np.argmin(distArray)\n",
    "            nClust[optCluster] += [clusters[i][j]]\n",
    "    return nClust\n",
    "\n",
    "# Complete K-Means algorithm\n",
    "def kMeans(data, dim):\n",
    "    clusters = initKMeans(data,dim) \n",
    "    oldCentres = None; centres = 0;\n",
    "    n = 0\n",
    "    while centres != oldCentres and n < 20:\n",
    "        oldCentres = centres\n",
    "        centres = kMeanCentUpdate(clusters,data) \n",
    "        clusters = kMeanUpdate(clusters, centres)\n",
    "        n += 1\n",
    "    return clusters, centres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two loss functions we will look at are\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$$loss(x) = \\frac{1}{m}\\Bigg(\\sum_{i=1}^{m} \\bigg({\\begin{cases}{0}& \\text{if }f(x^i) = y^i\\\\{1} & \\text{else}\\\\\\end{cases}}\\bigg)\\Bigg)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$$logLoss(x) = \\frac{1}{m}(\\sum_{i=1}^{m}y^{i}log(f(x^{i})) + (1-y^{i})log(1-f(x^i))$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print( \"The two loss functions we will look at are\")\n",
    "display(Math(r'loss(x) = \\frac{1}{m}\\Bigg(\\sum_{i=1}^{m} \\bigg({\\begin{cases}{0}& '\\\n",
    "             +r'\\text{if }f(x^i) = y^i\\\\{1} & \\text{else}\\\\\\end{cases}}\\bigg)\\Bigg)'))\n",
    "display(Math(r'logLoss(x) = \\frac{1}{m}(\\sum_{i=1}^{m}y^{i}log(f(x^{i})) + ' \\\n",
    "             + '(1-y^{i})log(1-f(x^i))'))\n",
    "\n",
    "def loss(w, beta, centres, trainData, trainTarget,actFunct):\n",
    "    cost = 0\n",
    "    for i in range(len(trainData)):\n",
    "        cost += sum([abs(classFunct(f(trainData[i], beta, w, centres,actFunct))[j] - \\\n",
    "                         trainTarget[i][j]) for j in range(len(trainTarget[i]))])\n",
    "    return float(cost)/float(2*len(trainData))\n",
    "\n",
    "def logLoss(w, beta, centres, trainData, trainTarget,actFunct):\n",
    "    cost = 0\n",
    "    for i in range(len(trainData)):\n",
    "        cost += abs(sum([(trainTarget[i][j]*np.log(f(trainData[i],beta,w,centres,\n",
    "                                                     actFunct)[j]) + \\\n",
    "                 (1 - trainTarget[i][j])* \\\n",
    "                          np.log(f(trainData[i],beta,w,centres,actFunct)[j])) \\\n",
    "                 for j in range(len(trainTarget[i]))]))\n",
    "    return float(cost) / float(len(trainData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradDescent(weighting, beta, centres, trainData, targetData, learnRate, actFunct):\n",
    "    if actFunct == \"RW\":\n",
    "        h = hRickerWavelet\n",
    "    else:\n",
    "        h = hGauss\n",
    "    wGrad = np.zeros((len(weighting),len(weighting[0])))\n",
    "    for point in range(len(trainData)):\n",
    "        F = f(trainData[point],beta,weighting,centres,actFunct)\n",
    "        Y = targetData[point]\n",
    "        diff = [tar - pred for pred,tar in zip(F,Y)]\n",
    "        for i in range(len(wGrad)):\n",
    "            for j in range(len(wGrad[1])):\n",
    "                wGrad[i][j] += -h(trainData[point], centres[j], beta[j])*diff[i]\n",
    "    w_ = np.zeros((len(weighting),len(weighting[1])))\n",
    "    for i in range(len(weighting)):\n",
    "        w_[i] = [origW - learnRate*wIter for origW,wIter in zip(weighting[i],wGrad[i])]\n",
    "    return w_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradDescentM(weighting, beta, centres, trainData, targetData, learnRate, eps, \\\n",
    "                 vList, actFunct):\n",
    "    if actFunct == \"RW\":\n",
    "        h = hRickerWavelet\n",
    "    else:\n",
    "        h = hGauss\n",
    "    wGrad = np.zeros((len(weighting),len(weighting[0])))\n",
    "    for point in range(len(trainData)):\n",
    "        F = f(trainData[point],beta,weighting,centres,actFunct)\n",
    "        Y = targetData[point]\n",
    "        diff = [tar - pred for pred,tar in zip(F,Y)]\n",
    "        for i in range(len(wGrad)):\n",
    "            for j in range(len(wGrad[1])):\n",
    "                wGrad[i][j] += -h(trainData[point], centres[j], beta[j])*diff[i]\n",
    "    w_ = np.zeros((len(weighting),len(weighting[1])))\n",
    "    for i in range(len(weighting)):\n",
    "        w_[i] = [origW - learnRate*wIter for origW,wIter in zip(weighting[i],wGrad[i])]\n",
    "    return w_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent Optimisation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def optimiseGradDesc(weighting,beta,centres,trainData,targetData,learnRate, \\\n",
    "                     maxTime,actFunct):\n",
    "    iterations = 0; TOL = 10e10; optList = [10e10, None, None]\n",
    "    h = []; n = 1\n",
    "    startTime = time.time()\n",
    "    while time.time() < (startTime + maxTime): \n",
    "        weighting=gradDescent(weighting,beta,centres,trainData,targetData,\\\n",
    "                              learnRate,actFunct)\n",
    "        TOL = loss(weighting,beta,centres,trainData,targetData,actFunct)\n",
    "        iterations += 1 \n",
    "        elapsedTime = time.time() - startTime\n",
    "        if elapsedTime > (n*maxTime)/5:\n",
    "            print(\"At \" + str(round(elapsedTime,4)) +\" seconds and \"+str(iterations) + \\\n",
    "                  \" iterations, the TOL is \" + str(round(TOL,6)))\n",
    "            n += 1\n",
    "        if optList[0] > TOL:\n",
    "            optList = [TOL, iterations, weighting]\n",
    "        if (optList[1]+300) < iterations:\n",
    "            print(\"RBF converged to min at iteration n = \"+str(optList[1])+\" with a \"\\\n",
    "                  + \"TOL of \" + str(round(optList[0],6)))\n",
    "            return optList, h\n",
    "        h += [[elapsedTime, TOL]]\n",
    "    print(\"Optimal optimisation at n = \"+str(optList[1])+\" of TOL: \"\\\n",
    "          +str(round(optList[0],6)))\n",
    "    return optList, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def optimiseSGDa(weighting,beta,centres,trainData,targetData,learnRate,batchSize, \\\n",
    "                maxTime,actFunct):\n",
    "    iterations = 0; TOL = 1; optList = [1, 10e10, None]\n",
    "    h = []; n = 10e10; startTime = time.time(); m = 1\n",
    "    while time.time() < (startTime + maxTime):\n",
    "        if (n+batchSize - len(trainData)) > 0:\n",
    "            trainDataIndex = range(len(trainData))\n",
    "            rnd.shuffle(trainDataIndex)\n",
    "            trainData = [trainData[i] for i in trainDataIndex]\n",
    "            targetData = [targetData[i] for i in trainDataIndex]\n",
    "            n = 0\n",
    "        weighting = gradDescent(weighting,beta,centres,trainData[n:n+batchSize], \\\n",
    "                                targetData[n:n+batchSize],learnRate,actFunct)\n",
    "        iterations += 1; n += batchSize\n",
    "        elapsedTime = time.time() - startTime\n",
    "        if elapsedTime > (m*maxTime)/5:\n",
    "            print(\"At \" + str(round(elapsedTime,4)) +\" seconds and \"+str(iterations) + \\\n",
    "                  \" iterations, the TOL is \" + str(round(TOL,6)))\n",
    "            m += 1\n",
    "            TOL = loss(weighting,beta,centres,trainData,targetData,actFunct)\n",
    "        if optList[0] > TOL:\n",
    "            optList = [TOL, iterations, weighting]\n",
    "        if (optList[1]+500) < iterations:\n",
    "            print(\"RBF converged to min at iteration = \"+str(optList[1])+\" with a TOL of \"\\\n",
    "                 + str(round(optList[0],6)))\n",
    "            return optList, h\n",
    "        h += [[elapsedTime, TOL]]\n",
    "    print(\"Optimal optimisation at n = \"+str(optList[1])+\" of TOL: \"\\\n",
    "          +str(round(optList[0],6)))\n",
    "    return optList, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def optimiseSGDb(weighting,beta,centres,trainData,targetData,learnRate,batchSize, \\\n",
    "                maxTime,actFunct):\n",
    "    iterations = 0; TOL = 1; optList = [10e10, 1 , None]; finalOptList = []\n",
    "    n = 10e10; startTime = time.time(); m = 1\n",
    "    while time.time() < (startTime + maxTime):\n",
    "        if (n+batchSize - len(trainData)) > 0:\n",
    "            trainDataIndex = range(len(trainData))\n",
    "            rnd.shuffle(trainDataIndex)\n",
    "            trainData = [trainData[i] for i in trainDataIndex]\n",
    "            targetData = [targetData[i] for i in trainDataIndex]\n",
    "            n = 0\n",
    "        weighting = gradDescent(weighting,beta,centres,trainData[n:n+batchSize], \\\n",
    "                                targetData[n:n+batchSize],learnRate,actFunct)\n",
    "        iterations += 1; n += batchSize\n",
    "        if time.time() < (startTime + maxTime - 10):\n",
    "            finalOptList += [[iterations, weighting]]\n",
    "    print(\"Training Complete\")\n",
    "    finalOptList = finalOptList[:5]\n",
    "    lossList = []\n",
    "    for i in range(len(finalOptList)):\n",
    "        lossList += [loss(finalOptList[i][1],beta,centres,trainData,\\\n",
    "                       targetData,actFunct)]\n",
    "        print(1)\n",
    "    minLVal = np.argmax(lossList)\n",
    "    print lossList[minLVal]\n",
    "    print(\"Optimisation finished at n = \"+str(finalOptList[minLVal][0])\\\n",
    "          +\" of TOL: \"+str(round(lossList[minLVal],6)))\n",
    "    return [lossList[minLVal], finalOptList[minLVal][0], finalOptList[minLVal][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def optimiseSGD(weighting,beta,centres,trainData,targetData,learnRate,batchSize, \\\n",
    "                maxTime,actFunct):\n",
    "    iterations = 0; TOL = 1; optList = [10e10, 1 , None]; finalOptList = []\n",
    "    n = 10e10; startTime = time.time(); m = 1\n",
    "    while time.time() < (startTime + maxTime):\n",
    "        if (n+batchSize - len(trainData)) > 0:\n",
    "            trainDataIndex = range(len(trainData))\n",
    "            rnd.shuffle(trainDataIndex)\n",
    "            trainData = [trainData[i] for i in trainDataIndex]\n",
    "            targetData = [targetData[i] for i in trainDataIndex]\n",
    "            n = 0\n",
    "        weighting = gradDescent(weighting,beta,centres,trainData[n:n+batchSize], \\\n",
    "                                targetData[n:n+batchSize],learnRate,actFunct)\n",
    "        iterations += 1; n += batchSize\n",
    "    print(\"Training Complete\")\n",
    "    #compLoss = loss(weighting,beta,centres,trainData,targetData,actFunct)\n",
    "    #print(\"Optimisation finished at n = \"+str(iterations)\\\n",
    "    #      +\" of TOL: \"+str(round(compLoss,6)))\n",
    "    return [iterations, weighting]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimiseSGD_M(weighting,beta,centres,trainData,targetData,learnRate,eps, \\\n",
    "                  batchSize,maxTime,actFunct):\n",
    "    iterations = 0; TOL = 1; optList = [10e10, 1 , None];\n",
    "    n = 10e10; startTime = time.time(); m = 1\n",
    "    velocity = 2*np.random.rand(len(weighting),len(weighting[0])) - 1\n",
    "    while time.time() < (startTime + maxTime):\n",
    "        if (n+batchSize - len(trainData)) > 0:\n",
    "            trainDataIndex = range(len(trainData))\n",
    "            rnd.shuffle(trainDataIndex)\n",
    "            trainData = [trainData[i] for i in trainDataIndex]\n",
    "            targetData = [targetData[i] for i in trainDataIndex]\n",
    "            n = 0\n",
    "        weighting,velocity = gradDescentM(weighting,beta,centres,trainData[n:n+batchSize], \\\n",
    "                                         targetData[n:n+batchSize],learnRate,eps,actFunct)\n",
    "        iterations += 1; n += batchSize\n",
    "    print(\"Training Complete\")\n",
    "    #compLoss = loss(weighting,beta,centres,trainData,targetData,actFunct)\n",
    "    #print(\"Optimisation finished at n = \"+str(iterations)\\\n",
    "    #      +\" of TOL: \"+str(round(compLoss,6)))\n",
    "    return [iterations, weighting]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBF Network Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Percentage Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function measures the accuracy of the RBF Network\n",
    "def classPercent(weighting,beta,centres,xData,yData,actFunct):\n",
    "    correct = 0\n",
    "    M = len(xData)\n",
    "    for i in range(M):\n",
    "        y = yData[i]\n",
    "        y_ = classFunct(f(xData[i],beta,weighting,centres,actFunct))\n",
    "        if (y_ == y).all():\n",
    "            correct += 1\n",
    "    percentage = round((float(correct)/float(M))*100,3)\n",
    "    print(\"RBF correctly identified \"+str(correct)+\" out of \"+str(M)+\" points\")\n",
    "    print(\"This gives an accuracy of \"+str(percentage)+\"% \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Opt Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def completeRBFNet(trainData, targetData, rbfIndex, learnRate, batchSize,maxIter):\n",
    "    # Dimension Reduction and dataset reduction\n",
    "    xTrain, yTrain = dimensionReduction(trainData,2000)\n",
    "    xTest, yTest = dimensionReduction(testData,1000)\n",
    "    # Normalize data\n",
    "    xTrain_ = xNorm(xTrain); xTest_ = xNorm(xTest)\n",
    "    yTrain_ = yNorm(yTrain); yTest_ = yNorm(yTest)\n",
    "    # Defining the rbf centres and beta value\n",
    "    _,centres = kMeans(xTrain_, rbfIndex)\n",
    "    beta = betaCalc(xTrain_, centres)\n",
    "    # Defining the initial weighting matrix\n",
    "    yLength = len(yTrain_[0])\n",
    "    weighting = np.random.random((yLength, rbfIndex))\n",
    "    # Optimising the RBF weight matrix\n",
    "    print(\"Optimising the RBF weights\")\n",
    "    print(\"\")\n",
    "    minTOL = 0.01\n",
    "    print(\"Deterministic Gradient Descent\")\n",
    "    optCondGD,optListGD = optimiseGradDesc(weighting,beta,centres,xTrain_,yTrain_,\\\n",
    "                                           learnRate[0],maxIter,\"Gauss\")\n",
    "    optWeightsGD = optCondGD[2]\n",
    "    print(\"\")\n",
    "    print(\"SGD w. Momentum\")\n",
    "    optCondSGD,optListSGD = optimiseSGD(weighting,beta,centres,xTrain_,yTrain_,\\\n",
    "                                           learnRate[1],batchSize,maxIter,\"Gauss\")\n",
    "    optWeightsSGD = optCondSGD[2]\n",
    "    # Plotting a graph of iterations against TOL value for both Gaussian and Ricker Wavelet\n",
    "    # activation functions\n",
    "    optListGDIndex = range(len(optListGD)) \n",
    "    nValsGD = [optListGD[i][0] for i in optListGDIndex]\n",
    "    TOLValsGD = [optListGD[i][1] for i in optListGDIndex]\n",
    "    plt.plot(nValsGD, TOLValsGD, 'r')\n",
    "    optListSGDIndex = range(len(optListSGD)) \n",
    "    nValsSGD = [optListSGD[i][0] for i in optListSGDIndex]\n",
    "    TOLValsSGD = [optListSGD[i][1] for i in optListSGDIndex]\n",
    "    plt.plot(nValsSGD, TOLValsSGD, 'g')\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"TOL\")\n",
    "    plt.show()\n",
    "    # Looking at the network's accuracy\n",
    "    print(\"\")\n",
    "    print(\"For Deterministic\")\n",
    "    print(\"Training data:\")\n",
    "    classPercent(optWeightsGD, beta, centres, xTrain_, yTrain_, \"Gauss\")\n",
    "    print(\"Test data:\")\n",
    "    classPercent(optWeightsGD, beta, centres, xTest_, yTest_, \"Gauss\")\n",
    "    print(\"\")\n",
    "    print(\"For SGD w. Momentum\")\n",
    "    print(\"Training data:\")\n",
    "    classPercent(optWeightsSGD, beta, centres, xTrain_, yTrain_, \"Gauss\")\n",
    "    print(\"Test data:\")\n",
    "    classPercent(optWeightsSGD, beta, centres, xTest_, yTest_, \"Gauss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "completeRBFNet(trainMNIST, testMNIST, 15, 5e-3, 10,180,\"Gauss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "completeRBFNet(trainMNIST, testMNIST, 15, 5e-3, 10,540,\"Gaus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "completeRBFNet(trainMNIST, testMNIST, 40, 5e-3, 10,600,\"Gauss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "completeRBFNet(trainMNIST, testMNIST, 40, 5e-3, 10,1200,\"Gauss\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
